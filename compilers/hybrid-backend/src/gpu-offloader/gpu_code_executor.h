#ifndef _H_gpu_code_executor
#define _H_gpu_code_executor

/* This header file contains the interface class the brokers GPU LPU execution between the host CPU and the GPU. The behavior
 * of the broker is as follows
 * 1. It is initialized with a context specific LpuBatchController that handles host to GPU data transfers and vice versa
 * 2. The caller invokes the broker for each LPU generated by the getNextLpu() routine
 * 3. If the number of LPUs and the memory consumed by them is less than the supported threshold then the broker just stores
 *    the LPU and passes control back to the caller
 * 4. Otherwise it executes the LPUs using the following protocol and then saves the LPU and returns control to the caller
 *	a. copy data parts from the host into the GPU (this involves an intermediate buffering on both sides; check the lpu
 *	   parts tracking library to understand the mechanism)
 *	b. invoke the virtual offloader function that will have a series of kernel calls as needed by the context and 
 *	   generated dynamically for the context specific broker sub-classes
 *	c. copy back updated data parts from the GPU card memory to the host LPU data parts
 *	d. cleanup any intermediate data structures generated within the host or the GPU card memory
 * There is a forceExecution() function that forces execution of partially filled batches to cover for terminal cases where 
 * LPU count is not divisible by the batch size.		
 */

#include "lpu_parts_tracking.h"
#include "../runtime/structure.h"

#include <fstream>
#include <vector>

// A statistics gathering class that records the total time spent handling different aspects of GPU LPU offloading
class OffloadStats {
  protected:
	double timeSpentStagingIn;
	double timeSpentExecution;
	double timeSpentStagingOut;
  public:
	OffloadStats();
	void addStagingInTime(double time) { timeSpentStagingIn += time; }
	void addExecutionTime(double time) { timeSpentExecution += time; }
	void addStagingOutTime(double time) { timeSpentStagingOut += time; }
	void describe(std::ofstream &logFile);
};

// Interface class for GPU LPU execution
class GpuCodeExecutor {
  protected:
	// Sometimes the GPU code executor works in a mode where the LPUs handed over to it be the host controller can be
	// multiplexed to GPU PPUs arbitrarily. Some other times, there must be strict controlling of what PPU executes what
	// LPUs. The submitNextLpus(vector<LPU*>) function below kepts LPUs for different GPU PPUs separate by having LPUs
	// LPUs for different PPUs at different indices of the vector, but whether the differential treatment should be
	// respected during the GPU execution is a decision made by the GPU-code-executor. The arbitrary LPU multiplexing 
	// approach provides better load balancing so it is the preferred default. Controlled location-sensitive LPU 
	// disbursement is the special case. The distinction between the two cases are made by this parameter. In the former
	// case the count is 1 so LPUs from the vector are all dumped into the same GPU LPU-Batch-Controller buffer. In the
	// latter case, the LPU buffers are kept distinct.  
	int distinctPpuCount;

	// These two parameters are needed to construct multidimensional LPU IDs inside the offloaded GPU kernels as opposed
	// to passing the IDs as a separate array
	std::vector<int*> *lpuCountVector;
	std::vector<Range> *lpuBatchRangeVector;

	LpuBatchController *lpuBatchController;
	std::ofstream *logFile;
	OffloadStats *offloadStats;
  public:
	GpuCodeExecutor(LpuBatchController *lpuBatchController, int distinctPpuCount);
	void setLpuCountVector(std::vector<int*> *lpuCountVector) { this->lpuCountVector = lpuCountVector; }
	void setLogFile(std::ofstream *logFile) { this->logFile = logFile; }
	
	// function to add a new LPU for batch execution in the GPU
	void submitNextLpu(LPU *lpu, int ppuGroupIndex);
	// function to force execution of partially filled batch of LPUs
	void forceExecution();

	// In some situation, it is a requirement that specific LPUs are executed by specific PPUs within the GPU. In such a
	// case, the host needs to distinguish what LPU has been generated for what PPU. So this function is provided to put
	// LPUs in specific vector indices to be processed by the lpuBathController in a PPU-location sensitive manner. The 
	// default implementation just picks non-NULL LPUs from the vector and submits them using the submitNextLpu function. 
	virtual void submitNextLpus(std::vector<LPU*> *lpuVector);

	// Subclasses should override these two functions to do any processing that needs to be done for all LPUs. For 
	// example, if scalar variables are accessed and modified within the compute stages that will execute inside the off-
	// loaded kernels then those variables should be copied in the GPU memory before the first batch of LPUs run and  
	// brought back to the host at the end of last batch execution. The default implementation of initialize only creates
	// a offload statistics accumulator and setup the batch LPU Range Vector, and the default implementation of cleanup 
	// tears down the CUDA context and prints the collected statistics. 
	virtual void initialize();
	virtual void cleanup();
  protected:
	// subclasses should provide implementation for this function should that invokes the proper sequence of GPU kernels 
	// to perform the compute stages relevant to the current context 
	virtual void offloadFunction() = 0;

	// To support transformation and re-transformation of an array's indices if index reordering partition functions have 
	// been used to create the data parts, metadata information about the entire part hierarchy is needed. In other words
	// host level LPU metadata is needed to process their descendant GPU LPUs. (Fortunately, the host level LPU is fixed
	// for all LPUs intended for a PPU Group in a particular batch because of the way we traverse the LPU hierarchy.) This
	// interface has been provided so that sub-classes can extract metadata from LPUs and pass that metadata to the GPU as
	// an additional kernel launch argument. Note that no corresponding LPU configuration metadata property is provided in
	// the class as the type of the metadata depends on the specific GPU-code-executor subclass.  
	virtual void extractAncestorLpuConfigs(LPU *lpu, int ppuGroupIndex) = 0;
  private:
	// this function implements the batch execution logic
	void execute();
	// after the execution of a batch, batch LPU ranges should be reset to prepare the executor for the next batch of LPUs
	void resetCurrentBatchLpuRanges();
};

#endif
