#ifndef _H_gpu_code_executor
#define _H_gpu_code_executor

/* This header file contains the interface class the brokers GPU LPU execution between the host CPU and the GPU. The behavior
 * of the broker is as follows
 * 1. It is initialized with a context specific LpuBatchController that handles host to GPU data transfers and vice versa
 * 2. The caller invokes the broker for each LPU generated by the getNextLpu() routine
 * 3. If the number of LPUs and the memory consumed by them is less than the supported threshold then the broker just stores
 *    the LPU and passes control back to the caller
 * 4. Otherwise it executes the LPUs using the following protocol and then saves the LPU and returns control to the caller
 *	a. copy data parts from the host into the GPU (this involves an intermediate buffering on both sides; check the lpu
 *	   parts tracking library to understand the mechanism)
 *	b. invoke the virtual offloader function that will have a series of kernel calls as needed by the context and 
 *	   generated dynamically for the context specific broker sub-classes
 *	c. copy back updated data parts from the GPU card memory to the host LPU data parts
 *	d. cleanup any intermediate data structures generated within the host or the GPU card memory
 * There is a forceExecution() function that forces execution of partially filled batches to cover for terminal cases where 
 * LPU count is not divisible by the batch size.		
 */

#include "lpu_parts_tracking.h"
#include "../runtime/structure.h"

#include <fstream>
#include <vector>

// A statistics gathering class that records the total time spent handling different aspects of GPU LPU offloading
class OffloadStats {
  protected:
	double timeSpentStagingIn;
	double timeSpentExecution;
	double timeSpentStagingOut;
  public:
	OffloadStats();
	void addStagingInTime(double time) { timeSpentStagingIn += time; }
	void addExecutionTime(double time) { timeSpentExecution += time; }
	void addStagingOutTime(double time) { timeSpentStagingOut += time; }
	void describe(std::ofstream &logFile);
};

// Interface class for GPU LPU execution
class GpuCodeExecutor {
  protected:
	// these two parameters are needed to construct multidimensional LPU IDs inside the offloaded GPU kernels as opposed
	// to passing the IDs as a separate array
	int *lpuCount;
	Range currentBatchLpuRange;

	LpuBatchController *lpuBatchController;
	std::ofstream *logFile;
	OffloadStats *offloadStats;
  public:
	GpuCodeExecutor(LpuBatchController *lpuBatchController);
	void setLpuCount(int *lpuCount) { this->lpuCount = lpuCount; }
	void setLogFile(std::ofstream *logFile) { this->logFile = logFile; }
	
	// function to add a new LPU for batch execution in the GPU
	void submitNextLpu(LPU *lpu);
	// function to force execution of partially filled batch of LPUs
	void forceExecution();

	// In some situation, it is a requirement that specific LPUs are executed by specific PPUs within the GPU. In such a
	// case, the host needs to distinguish what LPU has been generated for what PPU. So this function is provided to put
	// LPUs in specific vector indices to be processed by the lpuBathController in a PPU-location sensitive manner. The 
	// default implementation just picks non-NULL LPUs from the vector and submits them using the submitNextLpu function. 
	virtual void submitNextLpus(std::vector<LPU*> *lpuVector);

	// Subclasses should override these two functions to do any processing that needs to be done for all LPUs. For 
	// example, if scalar variables are accessed and modified within the compute stages that will execute inside the off-
	// loaded kernels then those variables should be copied in the GPU memory before the first batch of LPUs run and  
	// brought back to the host at the end of last batch execution. The default implementation of initialize only creates
	// a offload statistics accumulator, and the default implementation of cleanup tears down the CUDA context and prints
	// the collected statistics. 
	virtual void initialize();
	virtual void cleanup();
  protected:
	// subclasses should provide implementation for this function should that invokes the proper sequence of GPU kernels 
	// to perform the compute stages relevant to the current context 
	virtual void offloadFunction() = 0;
  private:
	// this function implements the batch execution logic
	void execute();
};

#endif
